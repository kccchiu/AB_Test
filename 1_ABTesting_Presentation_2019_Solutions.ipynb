{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFWJywnyNNU1"
   },
   "source": [
    "\n",
    "<img src=\"../Resources/Images/Slides-Logo.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<center><a href='https://drive.google.com/open?id=1DGOnHeq1RgWlEaRu69ZUYgZAWHKKniR9'>by Raul Maldonado</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQxhpsbUNNU6"
   },
   "source": [
    "# 1.0 The Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-l6TxLxNNU7"
   },
   "source": [
    "## 1.1 Introduction\n",
    "\n",
    "* Resources\n",
    "\n",
    "    * **Repository Link:** [http://bit.ly/Raul-ABTesting-PyDataLA](http://bit.ly/Raul-ABTesting-PyDataLA)\n",
    "    * **Google Colab version** of [this Jupyter Noteobok](https://drive.google.com/file/d/1XbyBPUUV9_5r89iZ9IATtQD23Jr0e9J8/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWPFltn9NNU8"
   },
   "source": [
    "* About Me:\n",
    "\n",
    "    * Data Analyst @Autodesk.\n",
    "    \n",
    "    * Enjoying coffee, running, and working on side projects...whenever I can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odQ7zJoTNNU9"
   },
   "source": [
    "____\n",
    "Today you'll have the opportunity to learn \"online experimentation\" scenario with A/B testing in Python.\n",
    "\n",
    "\n",
    "> Note: There exists **proprietary** A/B testing solutions like [Optimizely](https://www.optimizely.com/), [VWO](https://vwo.com/campaign/get-started/?utm_source=google&utm_medium=paid&utm_campaign=mof_search_brand_vwo_brand&utm_content=308583203468&utm_term=vwo&gclid=Cj0KCQjwv8nqBRDGARIsAHfR9wD7uaDnZRUDFKrXDtcn8jCv4v_dNhSRxzWsddKQAo0WuREO4phZ1PQaAopsEALw_wcB), [AB Tasty](https://www.abtasty.com/), [Google Optimize](https://optimize.google.com/optimize/home/). Moreover, there are *Open Source* solutions like Scipy, StatsModel, and more. \n",
    ">\n",
    ">However, this presentations shows the raw formualization for constructing an A/B test.\n",
    "\n",
    "**Information and data presented today is in no affiliation with Autodesk.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi6UihnONNU9"
   },
   "source": [
    "**Agenda:**\n",
    "\n",
    "1. Intro to Statistics [Pre]\n",
    "2. Overview of AB Testing\n",
    "3. AB Testing Implementation\n",
    "4. Results\n",
    "5. Ending & Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI6xi6-aNNU_"
   },
   "source": [
    "Enjoy! :D\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iSrZyEDgNNVA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6235527da291>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faker'"
     ]
    }
   ],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f2dWIRayNNVF"
   },
   "outputs": [],
   "source": [
    "### Note: If package faker is not installed, \n",
    "##### comment and apply the below package.\n",
    "# !pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFNQGjFiNNVI"
   },
   "source": [
    "## 1.2 Scenario "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUSf0oFLNNVJ"
   },
   "source": [
    "\n",
    "### 1.2.1 Existing Process\n",
    "\n",
    "A user enter's a search into Google Search. \n",
    "\n",
    "Thereafter, they receive a particular set of results, including paid recommendations (Ads) at the top of their ranked search results.\n",
    "\n",
    "Now, let's say we have an existing Ad A implemented to potentially be on that particular search. The user from above may click on the Ad. To increase the odds of them clicking the Ad A, we have a particular **Call to Action** \"Click here, please!\".\n",
    "\n",
    "The Ad's activity from a user is logged either as a \"Click\" or a \"No Click\", like so\n",
    "\n",
    "| Date     | Campaign  | User Email       | Action  |\n",
    "|----------|---|---------------------|------------|\n",
    "| 7/1/2019 | B |fake.email@comkast.net   | No Click |\n",
    "| 7/1/2019 | A |real.email@goog1e.com | Click      |\n",
    "| 7/4/2019 | A |real1.email@yawhoooo.com | Click |\n",
    "\n",
    "> Notice Ad Campaign B. It's an equivalent Ad to A, with a different Call to Action. This is shown later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-0GwZ23NNVK"
   },
   "source": [
    "### 1.2.2 Experiment Goals \n",
    "\n",
    "From the Ad, we would hope to get users to enter our website. \n",
    "\n",
    "Now, there are motivations for a user to enter our site, ~~like seeing if the new campaign will generate more money for our business~~. However, we start off with something more straightforward leading into that portion of the lifecyle. Particularly, we want a user to enter our site, the first step.\n",
    "\n",
    "\n",
    "![Clicks](https://media.giphy.com/media/3ogwG8ByATNb5EOm8E/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Q9wFSVNNVL"
   },
   "source": [
    "Before proceding further into further steps of the A/B testing overview, let's review some statistical items using Ad Campaign A..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aD6Ccce1NNVM"
   },
   "source": [
    "## 1.3  Stats 101, a brief review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyvPYkZDNNVM"
   },
   "source": [
    "Some of the utilized Statistical concepts:\n",
    "\n",
    "| Name     | Definition          | \n",
    "|----------|-----------------------|\n",
    "| Mean ($\\mu$) | $\\tfrac{\\sum_{i=1}^N  X_i}{n}$  |\n",
    "| Variance $ (\\sigma^2$) | $ \\tfrac{\\sum_{i=1}^N {(x_i -\\mu)^2}}{N}$| \n",
    "| Standard Deviation ($\\sigma $) | $ \\sqrt{ \\tfrac{\\sum_{i=1}^N {(x_i -\\mu)^2}}{N}} $  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "goqGDxb3NNVN"
   },
   "outputs": [],
   "source": [
    "# Intermission--Create Fake Dataset:\n",
    "\n",
    "def campaign_period(startDate, endDate):\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    endDate_dt = datetime.strptime(endDate, '%m-%d-%Y')\n",
    "    startDate_dt = datetime.strptime(startDate,'%m-%d-%Y')\n",
    "    numberOfDays = (endDate_dt - startDate_dt).days\n",
    "    date_list = [endDate_dt - timedelta(days=x) for x in range(numberOfDays)]\n",
    "    date_list.append(startDate_dt)\n",
    "    \n",
    "    return(date_list)\n",
    "    #Inspiration via  https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python\n",
    "    \n",
    "def campaign_dataset_generator(campaignName = 'PersonDoe-Campaign2019-n', \\\n",
    "                               weightArray=[.5,.5], sample_size = 100, \\\n",
    "                               timeframe = ['10-1-2019', '12-03-2019']):\n",
    "    import random\n",
    "    from faker import Faker\n",
    "    faker = Faker()\n",
    "    actions = ['Click','No Click']\n",
    "    campaignActions = random.choices(actions,\\\n",
    "                        weights=weightArray,\\\n",
    "                        k=sample_size)\n",
    "    \n",
    "    campaignTimeFrame = campaign_period(timeframe[0],timeframe[1])\n",
    "    generatedScenario = [[random.choice(campaignTimeFrame), campaignName, faker.email(), i] for i in campaignActions]\n",
    "    return(generatedScenario)\n",
    "\n",
    "def campaign_df_generator(matrix,columns):\n",
    "    campaignsDataset = pd.DataFrame(matrix,\\\n",
    "                                columns = columns)\n",
    "    \n",
    "    campaignsDataset.sort_values(by='Date',\\\n",
    "                             ascending=True, inplace = True)\n",
    "    campaignsDataset = campaignsDataset.pivot_table(index=['Date','Campaign'],\\\n",
    "                                                columns='Action',\\\n",
    "                                                aggfunc='size',\\\n",
    "                                                fill_value=0).reset_index(drop=False)\n",
    "    \n",
    "    campaignsDataset[['Click','No Click']] = campaignsDataset[['Click','No Click']].astype(float)\n",
    "\n",
    "    campaignsDataset.rename_axis(None, axis=1, inplace=True)\n",
    "    \n",
    "    uniqueCamp =campaignsDataset['Campaign'].unique()\n",
    "    returnedObjects = []\n",
    "    \n",
    "    for element in uniqueCamp:\n",
    "        returnedObjects.append(campaignsDataset[campaignsDataset['Campaign'] == element])\n",
    "\n",
    "    return(returnedObjects)\n",
    "\n",
    "def express_campaign_df_generator(campaignList,weightMatrix, sample_size):\n",
    "    firstCamp = campaign_dataset_generator(campaignList[0],weightMatrix[0], sample_size)\n",
    "    secondCamp = campaign_dataset_generator(campaignList[1],weightMatrix[1], sample_size)\n",
    "    combinedCampaigns = [*firstCamp, *secondCamp]\n",
    "    # https://stackoverflow.com/questions/1720421/how-do-i-concatenate-two-lists-in-python\n",
    "    \n",
    "    columns = ['Date','Campaign','User_ID','Action']\n",
    "\n",
    "    dfs = campaign_df_generator(combinedCampaigns, columns)\n",
    "    \n",
    "    return(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hk4rPK4-NNVR"
   },
   "outputs": [],
   "source": [
    "def distribution_plot(series, series2=None, name_of_campaign = ''):\n",
    "\n",
    "    sns.distplot( series['CTR'], hist=True, kde=True, \\\n",
    "        kde_kws = {'shade': True, 'linewidth': 3})\n",
    "\n",
    "    plt.title(f'Campaign {name_of_campaign}\\'s Success Distribution')\n",
    "\n",
    "    plt.xlabel(series['CTR'].name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9tTiC3ENNVU"
   },
   "outputs": [],
   "source": [
    "## [Optional] Quick location to change the size for the following datasets\n",
    "size = 1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "4_gco6DpNNVX",
    "outputId": "a5dcb67b-2825-4669-c728-3d7816282521"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-017cdb04f5da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfirstCampaign_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcampaign_dataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcampaignName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcampaign1_name\u001b[0m\u001b[0;34m,\u001b[0m                                            \u001b[0mweightArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                            \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msecondCampaign_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcampaign_dataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcampaignName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcampaign2_name\u001b[0m\u001b[0;34m,\u001b[0m                                             \u001b[0mweightArray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m.53\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.47\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                             \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-993bb70a4bd7>\u001b[0m in \u001b[0;36mcampaign_dataset_generator\u001b[0;34m(campaignName, weightArray, sample_size, timeframe)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcampaign_dataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcampaignName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PersonDoe-Campaign2019-n'\u001b[0m\u001b[0;34m,\u001b[0m                                \u001b[0mweightArray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m                                \u001b[0mtimeframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'10-1-2019'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'12-03-2019'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mfaker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mfaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Click'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'No Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faker'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "campaign1_name = 'PersonDoe-Campaign2019-1'\n",
    "campaign2_name = 'PersonDoe-Campaign2019-2'\n",
    "\n",
    "\n",
    "firstCampaign_ds = campaign_dataset_generator(campaignName = campaign1_name, \\\n",
    "                                           weightArray = [.5, .5], \\\n",
    "                                           sample_size = size)\n",
    "\n",
    "secondCampaign_ds = campaign_dataset_generator(campaignName = campaign2_name, \\\n",
    "                                            weightArray = [.53, .47], \\\n",
    "                                            sample_size = size)\n",
    "\n",
    "# Concatenate matrices/arrays\n",
    "combinedCampaigns = [*firstCampaign_ds, *secondCampaign_ds]\n",
    "\n",
    "columns = ['Date','Campaign','User_ID','Action']\n",
    "\n",
    "\n",
    "dfs = campaign_df_generator(combinedCampaigns, columns)\n",
    "\n",
    "campaign1 = dfs[0].reset_index(drop=True)\n",
    "campaign2 = dfs[1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIXJYPY0NNVZ"
   },
   "outputs": [],
   "source": [
    "# Check out the first 3 rows of dataframe campaign1\n",
    "campaign1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DHgfYkTNNVf"
   },
   "outputs": [],
   "source": [
    "# Check out the first 3 rows of dataframe campaign2\n",
    "campaign2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wrwq7WENNVi"
   },
   "source": [
    "### 1.3.1 Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0Gsb3J2NNVj"
   },
   "source": [
    "Observing Ad A, the (Arithmetic) $\\text{Mean}$ $\\mu$ calculates the total sum of values divided by the the total count of values being summed up, \n",
    "or $\\mu = \\tfrac{\\sum_{i=1}^N {x_i}}{N}$\n",
    "\n",
    "\n",
    "E.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-c0W5Hh5NNVk"
   },
   "outputs": [],
   "source": [
    "# For some given set of values generated from the fake data we created, \n",
    "# we calculate the average of the first 5 \"Click\" values.\n",
    "\n",
    "firstFiveValues = campaign1.Click[:5]\n",
    "\n",
    "testMean_numerator = firstFiveValues.sum()\n",
    "testMean_denominator = firstFiveValues.size\n",
    "\n",
    "testMean = testMean_numerator / testMean_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QjT-ibCNNVn"
   },
   "outputs": [],
   "source": [
    "testMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iDrFFxhNNVq"
   },
   "outputs": [],
   "source": [
    "print(f\"We have the array {list(firstFiveValues)}. \\n\\nTaking the total sum, {testMean_numerator},\", \n",
    "      f\"and then dividing by the size {testMean_denominator}\", \n",
    "      f\", we get an average of...\\n{testMean_numerator}/{testMean_denominator} = {testMean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d3ZZz0pNNVs"
   },
   "source": [
    "### 1.3.2 Variance & Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJFYG_SDNNVt"
   },
   "source": [
    "Variance is the measure of spread of a given set of values. Alternatively, it can be formulaically said that it is the average of the squared differences from the mean, seen below.\n",
    "\n",
    "$ \\text{Variance} = \\tfrac{\\sum_{i=1}^N {(x_i -\\mu)^2}}{N}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQLVIMzXNNVu"
   },
   "outputs": [],
   "source": [
    "testVar_numerator_step_1 = [firstFiveValues - testMean]\n",
    "testVar_numerator_step_2 = np.power(testVar_numerator_step_1, 2)\n",
    "testVar_numerator = testVar_numerator_step_2.sum()\n",
    "\n",
    "testVar_denominator = (testMean_denominator)\n",
    "\n",
    "testVar = testVar_numerator/testVar_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srHWgvNeNNVx"
   },
   "outputs": [],
   "source": [
    "testVar_numerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfgnUZQDNNV0"
   },
   "source": [
    "\n",
    "Taking the squared root of the variance, we have a measure of spread in the same units as the spread. \n",
    "> That being said, we see that the Variance and Standard Deviation being related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pksmltmnNNV1"
   },
   "source": [
    "$\\sqrt{\\text{Variance}} = \\sqrt{\\sigma ^2} =  \\sigma  = \\sqrt{ \\tfrac{\\sum_{i=1}^N {(x_i -\\mu)^2}}{N}} = \\text{Standard Deviation} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_z2z4gMNNV2"
   },
   "outputs": [],
   "source": [
    "testStd = round(np.sqrt(testVar), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSATMWOjNNV5"
   },
   "outputs": [],
   "source": [
    "print(f\"We have array {list(firstFiveValues)}, with mean {testMean}.\",\n",
    "      f\" \\n\\nTaking summed squared difference from the mean, {testVar_numerator},\", \n",
    "      f\"and then dividing by the 'size - 1', {testVar_denominator},\", \n",
    "      f\"we get a variance of...\\n{testVar_numerator}/{testVar_denominator} = {testVar}.\\n\", \n",
    "      f\"Lastly, taking the square root of the variance, we get a Standard Deviation of...{testStd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_p_fzgENNV_"
   },
   "source": [
    "# 2.0 A/B Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRiOZ_dMNNWB"
   },
   "source": [
    "Now, let's come back to our A/B Testing discussion. \n",
    "\n",
    "Recall we had deployed two digital ad campaigns A and B. Moreover, recall Campaign A has some _Call to Action_ **\"Click here, please!\"**.\n",
    "\n",
    "Now, let Ad B be a replica of A, but with a subtle change to the Call to Action text, being **_\"Learn more here!\"_**. \n",
    "\n",
    "\n",
    "We want to understand the performance between each campaign. \n",
    "\n",
    "Should we just \"eye\" the performance of these two campaigns, and make a conclusion? If not, what's the more methodic/statistical approach to comparing the performances between these two Ads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7jN3p2mNNWC"
   },
   "source": [
    "One statistical method we can turn to is A/B Testing.\n",
    "\n",
    "What is A/B Testing?\n",
    "\n",
    "**A/B Testing** is \"[a randomized online experiment of two variants, A and B.](https://en.wikipedia.org/wiki/A/B_testing)” This test quantitatively compares two samples with a single \"metric of choice\" in evaluation to determine if there exists a statistical significance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIBJ6ddBNNWC"
   },
   "source": [
    "They are essentially a modern, online, adaptation of statistical experiment frameworks called Hypothesis tests. \n",
    "\n",
    "> Commentary: There are those that surley do believe A/B tests are distinct from Hypothesis tests, given the difference in nature of experimentation setup from Academia, etc. \n",
    ">\n",
    ">However, there are many that believe  A/B tests was borrowed from the Statistics world, and labeled an edgy/cool name to distinguish iteself. #Marketing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Dd0qgyQNNWD"
   },
   "source": [
    "![Not Lame](../Resources/Images/not_lame_meme.png)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vupV0cfNNWD"
   },
   "source": [
    "## 2.1 A/B Testing Background\n",
    "\n",
    "A \"high-level\" overview of A/B testing can be illustrated in the following:\n",
    "\n",
    "For some test metric, \n",
    "\n",
    "![AB Testing Format](../Resources/Images/ABTesting-Format.png)\n",
    "\n",
    "(**Source**: [“Probably Overthinking It” by Allen Downey](http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK9dh9YMNNWE"
   },
   "source": [
    "In more depth for the test-statistic step:\n",
    "\n",
    "If you observe the following diagram of choices below, notice one has to select the right calculation & criteria for their experiment. They have to consider assumptions like data distribution, data types, number of samples, and other assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAGzy9mzNNWE"
   },
   "source": [
    "Student's introduction to Statistics can involve the following tree of decisions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GATgiOr7NNWF"
   },
   "source": [
    "![Tree](../Resources/Images/AB-Testing-Choices-Tree.png)\n",
    "\n",
    "[Source](https://bloomingtontutors.com/blog/when-to-use-the-z-test-versus-t-test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zsgYj-WNNWF"
   },
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5gmfAyFNNWG"
   },
   "source": [
    "> **Commentary:** Other mentions or comparisons between Z-test & T-test, $\\chi^2$ test, and other items from the above may be covered in the Appendix section, or referenced repository, seen in the **\"Additional Information\"** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW_lLNHtNNWG"
   },
   "source": [
    "![Sleepy](../Resources/Images/tired_gif.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrb4cjM6NNWG"
   },
   "source": [
    "Now, let's get started with a practice run of A/B Testing.\n",
    "\n",
    "We keep in mind of the following procedure:\n",
    "\n",
    "1. State your Hypothesis \n",
    "2. Statistical Assumptions\n",
    "3. Define and collect control group information\n",
    "4. Identify you Minimum Detectable Effect, Sample Size, and more\n",
    "5. Analyze\n",
    "6. Conclusion\n",
    "\n",
    "> Commentary: Carefully implementing these procedures in one's experiment, we ensure a resound testing framework & reduced risk of incorrect results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVnjGXFkNNWH"
   },
   "source": [
    "## 2.2 Testing, Testing..1,2,3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXYnqIKBNNWI"
   },
   "source": [
    "\n",
    "### 2.2.0.1 Metric Of Choice\n",
    "\n",
    "Recall we want to evaluate the performances of Ads A & B for users entering our website. Seeing these changes of the click interaction of our Ads, we want to obser the the Click Through Rate (CTR) performance. Therefore, CTR is our Metric of Choice for our AB Test.\n",
    "\n",
    "Let the CTR proportion $\\hat{p}_i$ be defined as $\\hat{p}_i = \\tfrac{\\text{Total Number of Successes}}{\\text{Total Number of Events}} = \\tfrac{x_i}{n_i} $ \n",
    "\n",
    "where \n",
    "\n",
    "$x_i$ are the successes \n",
    "& \n",
    "$n_i$ is the total count of each sample.\n",
    "\n",
    "> Note: those who 'click' an Ad is a binary metric, a user either clicks the button at some point, or she/he does not.\n",
    ">\n",
    "> Note: There are other evaluation metrics for variant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XaWwLSDNNWI"
   },
   "source": [
    "## 2.2.1 State Your Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdUQZWX-NNWN"
   },
   "source": [
    "We would like to observe if there is a difference in performance between the two Ads. Particularly, we want to observe if there is a statistically significant difference in their CTR performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTae5fHbNNWN"
   },
   "source": [
    "Let $\\hat{p_1}$ = $\\text{CTR}_{\\text{Campaign A}}$  and $\\hat{p_2}$  = $\\text{CTR}_{\\text{Campaign B}}$ be the CTR proportions for campaign A & B, respectively.\n",
    "\n",
    "where $\\hat{p}:= \\tfrac{\\text{# of Clicks}}{\\text{Total # of events}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0Je24G9NNWO"
   },
   "source": [
    "We state:\n",
    "\n",
    "$H_0: \\hat{p_1} = \\hat{p_2}$ , or $d = \\hat{p_1} - \\hat{p_2} = 0$\n",
    "\n",
    "\n",
    "$H_A: d =\\hat{p_1} - \\hat{p_2} \\neq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKghHzlrNNWP"
   },
   "source": [
    "We would establish a Level of Significance, $\\alpha$. \n",
    "\n",
    "Set $\\alpha = .05$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEwXa2cONNWP"
   },
   "outputs": [],
   "source": [
    "alpha = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1kXN422NNWR"
   },
   "source": [
    "From the notation above,\n",
    "\n",
    "We are saying for a default assumption that there is no difference between Ad Campaign A and Campaign B. However, the Alternative Hypothesis $H_A$ there is a significant difference between Ad Campaign A and B.\n",
    "\n",
    "Although, we had choosen 1 of 3 hypotheses to consider in a test. \n",
    "\n",
    "What are the other permutations for stating our initial hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq7oqcxmNNWR"
   },
   "source": [
    "### 2.2.1.1 The statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4LswtLeNNWS"
   },
   "source": [
    "In a different context, under the assumption of our $H_0: \\mu = \\text{value}$,\n",
    "\n",
    "Let $H_0$ be our Null Hypothesis, the statement saying the comparison between the average of a population and a sample value has no observed differences, statistical differences.\n",
    "\n",
    "Let $H_A$, $H_1$, be our Alternative Hypothesis, the statement saying the comparison between the average of a population and a sample value does has observerd statistical difference.\n",
    "\n",
    "However, we can also say:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9ubk8phNNWS"
   },
   "source": [
    "![Types of Test](../Resources/Images/two_tailed_test.png)\n",
    "\n",
    "[Source](https://www.fromthegenesis.com/difference-between-one-tail-test-and-two-tail-test/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA3ZIcPONNWT"
   },
   "source": [
    "And it should be noted here that we are **not trying to prove , $H_a$, that there is a significant difference between the groups**. \n",
    "\n",
    "Rather, we are observing from the standpoint of no difference to see if one exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYzpKa-vNNWT"
   },
   "source": [
    "\n",
    "> Commentary: Imagine being in a state of being leanient to be proven guilty until proven guilty, compared to innocent until proven so. Similar to being a judge, one is highly recommended to approach an evaluation under standpoint of neutrality.\n",
    "\n",
    "> ![Court](../Resources/Images/innocent_court.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9yoq20CNNWU"
   },
   "source": [
    "But wait, what about  $\\alpha$ value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RF1nF2gkNNWU"
   },
   "outputs": [],
   "source": [
    "# x1 = np.linspace(-4,20,100)\n",
    "# mean1 = np.mean(x1)\n",
    "# std1 = np.std(x1)\n",
    "\n",
    "# y1 = scipy.stats.norm.pdf(x1,mean1,std1)\n",
    "\n",
    "\n",
    "# x2 = np.linspace(6,30,100)\n",
    "# mean2 = np.mean(x2)\n",
    "# std2 = np.std(x2)\n",
    "\n",
    "# y2 = scipy.stats.norm.pdf(x2,mean2,std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVhV30fxNNWX"
   },
   "outputs": [],
   "source": [
    "# deltaX = np.linspace(min(x2), z, 100)\n",
    "# deltaY = y2-y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnJzD5mUNNWZ"
   },
   "outputs": [],
   "source": [
    "# norm = scipy.stats.norm\n",
    "\n",
    "\n",
    "# pepe_params = norm.fit(x1)\n",
    "# modern_params = norm.fit(x2)\n",
    "\n",
    "# xmin = min(x1.min(), x2.min())\n",
    "# xmax = max(x1.max(), x2.max())\n",
    "# x = np.linspace(-15, 35, 100)\n",
    "\n",
    "# pepe_pdf = norm(*pepe_params).pdf(x)\n",
    "# modern_pdf = norm(*modern_params).pdf(x)\n",
    "# y = np.minimum(modern_pdf, pepe_pdf)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_ylim(min(modern_pdf),max(modern_pdf))\n",
    "# ax.axvline(1.96 * (np.std(x)/(np.sqrt(100)-1)) + np.mean(x) , color='red')\n",
    "# ax.plot(x, pepe_pdf, color='blue')\n",
    "# ax.plot(x, modern_pdf, color='orange')\n",
    "# ax.fill_between(x, y, color='red', alpha=0.3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XJv1rJiNNWa"
   },
   "source": [
    "### 2.2.1.2 $\\alpha$, and Outcomes from Decisions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psfDpcYJNNWb"
   },
   "source": [
    "$\\alpha$ is our **Level of Significance**. \n",
    "\n",
    "It is the the probability of the study rejecting the null hypothesis $H_0$, given that the null hypothesis were true.\n",
    " \n",
    "Similarly, we have the Confidence Level, $1-\\alpha$. The Confidence Level is our probability of failing to reject the null hypothesis, given that it is true. \n",
    "\n",
    "However, there are two more calculated scenarios one must consider from their decision, $\\beta$ & $1-\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nBcZ5JyNNWc"
   },
   "source": [
    "$\\beta$ is the probability of failing to reject the null hypothesis $H_0$, when $H_0$ is False.\n",
    "\n",
    "> That is, what is the probability of sticking to our ways given that there is statistical evidence an alternative scenario exists?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L03CFRGmNNWd"
   },
   "source": [
    "And the last probability to mention is $1- \\beta$, which is the probability of rejecting the null hypothesis $H_0$, when $H_0$ is false. This is called the **Power** of a test. \n",
    "\n",
    "> Commentary: Traditionally the Power is expected to be .80, like the default of $\\alpha = 0.05$\n",
    "\n",
    "The following is a visual representation of these decisions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAha5k_GNNWe"
   },
   "source": [
    "In matrix form,\n",
    "\n",
    "![AB Testing Decisions](../Resources/Images/ab_testing_choices.png)\n",
    "\n",
    "[Source](https://www.abtasty.com/blog/type-1-and-type-2-errors/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xagVP6p4NNWf"
   },
   "source": [
    "\n",
    "![MDE n'Alpha graph](../Resources/Images/decision_outcomes_ab_test.jpg)\n",
    "\n",
    "[Source](https://upload.wikimedia.org/wikipedia/commons/thumb/3/30/H0_h1_fehler.jpg/512px-H0_h1_fehler.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vC2Sqp5fNNWg"
   },
   "source": [
    "### 2.2.1.3 Basic Assumptions <span style='color:red'>*</span>\n",
    "\n",
    "From the collected samples for this type of targeted data, we assume the following conditions:\n",
    "\n",
    "* Each click event is independent from one another\n",
    "\n",
    "* We have a Simple Random Sample\n",
    "\n",
    "* A user either clicks or does not click (leaves an impression)\n",
    "\n",
    "* Assumption of Central Limit Theorem. (Allows for the assumption of a normal distribution)\n",
    "\n",
    "    * With an appropriate traffic size, the binomial-like distribution of this scenario reaches a Standard Normal (Gaussian) Distribution\n",
    "\n",
    "* We have a consideration of invariant and variant metrics for our experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT1BN75PNNWh"
   },
   "source": [
    "#### 2.2.2 Data Transformation [Misc.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "ws_OkXICNNWi",
    "outputId": "633a952d-9027-4839-8f43-c9df28112d03"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c1464639ce08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'DataFrame Schema:\\n{campaign1.columns.tolist()}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'campaign1' is not defined"
     ]
    }
   ],
   "source": [
    "# # Confirm Dataset Schema\n",
    "print(f'DataFrame Schema:\\n{campaign1.columns.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOR_qED5NNWl"
   },
   "source": [
    "We observe that the dataframe contains dimension information, along with measures like Clicks and No Clicks.\n",
    "\n",
    "**Question**\n",
    "\n",
    "Recall the CTR formula from before. \n",
    "\n",
    "Create a column 'CTR' in the both dataframe's _campaign1_ and _campaign2_\n",
    "\n",
    "**CTR** $:= \\tfrac{\\text{?}_1}{\\text{?}_2}$\n",
    "\n",
    "Hint: Our \"Metric of Choice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nZGmr7fNNWm"
   },
   "outputs": [],
   "source": [
    "# Calculate the CTR for Ad Campaign 1's column campaign2['CTR']\n",
    "campaign1['CTR'] = campaign1['Click'] / (campaign1['No Click'] + campaign1['Click'])\n",
    "\n",
    "# Calculate the CTR for Ad Campaign 2's column campaign2['CTR']\n",
    "campaign2['CTR'] = campaign2['Click'] / (campaign2['No Click'] + campaign2['Click']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpDygy3eNNWn"
   },
   "source": [
    "The daily CTR distributions are as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rkxCTgJNNWn"
   },
   "outputs": [],
   "source": [
    "# Establish the 1st campaign's distribution\n",
    "distribution_plot(series = campaign1, name_of_campaign= campaign1_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE3WxDpbNNWp"
   },
   "outputs": [],
   "source": [
    "# Establish the 2nd campaign's distribution\n",
    "\n",
    "distribution_plot(series = campaign2, name_of_campaign= campaign2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkN6vDOjNNWr"
   },
   "outputs": [],
   "source": [
    "# CTR Proportion Calculations\n",
    "\n",
    "n1 = (campaign1['No Click'].sum() + campaign1['Click'].sum())\n",
    "# p = x/n\n",
    "p1 = campaign1['Click'].sum() / n1\n",
    "\n",
    "n2 = (campaign2['No Click'].sum() + campaign2['Click'].sum())\n",
    "# p = x/n\n",
    "p2 = campaign2['Click'].sum() / n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZPk-3LENNWt"
   },
   "outputs": [],
   "source": [
    "print(f'Ad Campaign 1: CTR = {p1}, from Total Occurences are {n1}')\n",
    "\n",
    "print(f'Ad Campaign 2: CTR = {p2}, from Total Occurences are {n2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LcorEO3dNNWw"
   },
   "source": [
    "## 2.3 Analyze The Results <span style='color:red'>*</span>\n",
    "\n",
    "In the calculation options covered in the AB Testing overview portion, we saw that a calculation is considered and dependent on the nature of the question and type of metric calculated. In this particular case, we will be using the $t$ statistic b/t two binomial distributed groups, defined as \n",
    "\n",
    "$t= (\\tfrac{ \\hat{p_1} - \\hat{p_2} - 0 }{SE}) = \\tfrac{ d - 0 }{SE}$, \n",
    "\n",
    "where the standard error $SE = \\sqrt{(\\tfrac{\\hat{p_1} (1-\\hat{p_1})}{n_1}) + (\\tfrac{\\hat{p_2} (1-\\hat{p_2})}{n_2})}$\n",
    "\n",
    "> Note: \n",
    "> 1. When comparing between two groups, the notation $d:=\\hat{p_1} - \\hat{p_2}$ is introduced to simplify formula.\n",
    "> 2. This is considered a independent t-test with equal variance, and not a dependent t-test w or without equal metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILidyqj3NNWw"
   },
   "source": [
    "\n",
    "Additionally, we set our Degrees of Freedom for these two variants, defined as $DoF := (n_1 + n_2 -2)$. \n",
    "\n",
    "Using this DoF, we calculate the estimated $t^*$ value as a our threshold level for determining statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j06cWaYXNNWy"
   },
   "source": [
    "### 2.3.1 Manual Approach\n",
    "\n",
    "For the following function, calculate the difference d, standard error, and t-statistic. Moreover, determine the resulting decision for this particular case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wa9DTFF2NNWz"
   },
   "outputs": [],
   "source": [
    "def ind_t_test(group1, group2, alpha = 0.05, output_bool=False, state_conclusion =False, express=False):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ind_t_test Function for manual calculation of Independent T Tests for CTR.\n",
    "    \n",
    "    Parameters:\n",
    "    * group1: Test Group #1 (Dataframe)\n",
    "    * group2: Test Group #2 (Dataframe)\n",
    "    * alpha: Alpha is the Test of Significance\n",
    "    * output_bool:\n",
    "    * state_conclusion:\n",
    "    * express:\n",
    "    \n",
    "    \n",
    "    Return:\n",
    "    * 1x2 Array of Click Through Rate (CTR) values. \n",
    "    * 1x2 Array of total N for each campaign. \n",
    "    * 1x2 Array of Standard Errors for each campaign. \n",
    "    * Standard Error value.\n",
    "    * T-Statistic value.\n",
    "    * T-Critical value.\n",
    "    * Confidence Interval   \n",
    "    * Calculated P-Value\n",
    "    '''\n",
    "    \n",
    "    # Step 1: Take the proportions of provided datasets:\n",
    "    ## Create the proportion calculations: p_1, p_2 for the\n",
    "    ## difference d. \n",
    "    ## The first calculation step for our test.\n",
    "    \n",
    "    n1 = (group1['No Click'].sum() + group1['Click'].sum())\n",
    "    p1 = group1['Click'].sum() / float(n1)\n",
    "\n",
    "    n2 = (group2['No Click'].sum() + group2['Click'].sum())\n",
    "    p2 = group2['Click'].sum()  / float(n2)\n",
    "\n",
    "    d= p2 - p1\n",
    "\n",
    "    # Step 2. Obtain the Standard Deviation:\n",
    "    #    - Combined group Standard errors  \n",
    "    ## Calculate the Standard Error for each proportions.\n",
    "    ## This allows us to set up for the t-statistic calculation.\n",
    "    p_overall = (group1['Click'].sum() + group2['Click'].sum() )/ (n1+n2)\n",
    "    \n",
    "    # Term 1\n",
    "    ## Calculate the first term p_1 * (1-p_1) /n_1\n",
    "    se1 = (p_overall * (1 - p_overall))/n1\n",
    "    \n",
    "    # Term 2\n",
    "    ## Calculate the first term p_2 * (1-p_2) /n_2\n",
    "    se2 = (p_overall * (1- p_overall))/n2\n",
    "    \n",
    "    ## Terms being squared for final result.\n",
    "    ### SE = sqrt( SE_1 + SE_2)\n",
    "    standardError = np.sqrt(float(se1) + float(se2))\n",
    "    \n",
    "    print('SE 1:', standardError)\n",
    "\n",
    "    #Step 3 t-statistic\n",
    "    ## T statistic calculation.\n",
    "    ### d / Standard Error, where d:= p2-p1\n",
    "    tStatistic = (d-0) / standardError\n",
    "\n",
    "    # Degrees of Freedom\n",
    "    ### (n_1-1) + (n_2-1) = (n_1 + n_2 -2)\n",
    "    dof = (n1 + n2 - 2.0)\n",
    "    \n",
    "    \n",
    "    # Critical T Value Test Statistic\n",
    "    criticalValue = scipy.stats.t.ppf(1.0 - alpha, df = dof)\n",
    "\n",
    "\n",
    "    # Confidence Interval\n",
    "    ### Tip: We'd like to avoid value of 0 in this CI\n",
    "    confInt = [d - (criticalValue * standardError), d + (criticalValue * standardError)]\n",
    "\n",
    "    \n",
    "    # Second type of decision criteria: |t| >= |t^*|.\n",
    "    \n",
    "\n",
    "    # Step 4: Calculating p-value\n",
    "    ## Two Sided P Value, P( |t| >= |t^*|).\n",
    "    ### Calcualte the p-value using a Cumulative Density function\n",
    "    ### from Scipy's stats.t.cdf(t-test, DoF) function\n",
    "    \n",
    "    p_val = (1 - scipy.stats.t.cdf(abs(tStatistic), df \\\n",
    "                                   = (n1 + n2-2)) ) * 2.0\n",
    "    \n",
    "    if output_bool is True:\n",
    "        print('Analysis:\\n\\n')\n",
    "        print(f'Campaign {group1.Campaign[0]}\\'s CTR: {round(p1,4)}' \\\n",
    "              + f' with Standard Error {se1}.')\n",
    "        print(f'Campaign {group2.Campaign[3]}\\'s CTR: {round(p2,4)}' \\\n",
    "              + f' with Standard Error {se2}.\\n')\n",
    "        \n",
    "        print(f\"Confidence Interval {confInt}\")\n",
    "        print(f'T Statistic: {round(tStatistic, 2)}\\n')\n",
    "        \n",
    "        print(f'We have critical value t^* at {round(criticalValue, 2)}' + \\\n",
    "              f'\\nand p-value of {round(p_val, 2)}')\n",
    "        \n",
    "        print(f'\\n\\nComponents for variants Campaign {group1.Campaign[0]}\\'s \\n& ' + \\\n",
    "              f'Campaign {group2.Campaign[3]}\\'s, respectively:')\n",
    "        \n",
    "        print(f'Difference d: {d}')\n",
    "        \n",
    "        print(f'SE terms within SE calculation: {[se1,se2]}')\n",
    "        print(f'SE: {standardError}')\n",
    "        \n",
    "        print(f'Calcualted T-statistic: {tStatistic}')\n",
    "        print(f'T critical value: {criticalValue}')\n",
    "    \n",
    "    \n",
    "    #Step 5 Statemtent of declaration & Decision        \n",
    "    if state_conclusion is True:\n",
    "        if express is False:\n",
    "            # Restate our decision process\n",
    "            print('Conclusion:\\n\\n')\n",
    "\n",
    "            print(f'If the p-value is less than our defined alpha = {alpha}, then we' +\\\n",
    "                  ' reject the null hypothesis H_0.\\nIf not, then we fail to reject the' +\\\n",
    "                  ' null hypothesis H_0.')\n",
    "\n",
    "            print(f'Confidence Interval: {confInt}')\n",
    "            print(f'P-value: {p_val}')\n",
    "            print(f'Alpha: {alpha}')\n",
    "        \n",
    "        if p_val < alpha:\n",
    "            print('\\nWe reject the Null Hypothesis H_0')\n",
    "            print('Therefore, we can say that there is a statistical ' + \\\n",
    "            'difference between the two campaigns.')\n",
    "\n",
    "        else:\n",
    "            print('\\nWe fail to reject the Null Hypothesis H_0')\n",
    "            print('\\nTherefore, we can say that there is no statistical' + \\\n",
    "            ' significant difference between the two campaigns.')\n",
    "    return([p1,p2], [n1,n2], \\\n",
    "           [se1,se2], standardError, \\\n",
    "           tStatistic, criticalValue, \\\n",
    "           confInt, p_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkyDXvvHNNW1"
   },
   "source": [
    "If the calculated t-statistic $t$ > critical value $t^*$, then we reject the null hypothesis $H_0$, and accept the alternative hypothesis $H_A$.\n",
    "\n",
    "\n",
    "> Equivalently, if $p < \\alpha$, then we  reject the null hypothesis $H_0$, and accept the alternative hypothesis $H_A$.\n",
    "\n",
    "> Ok.. what's a **P-value**? P-value is the probability of obtaining an effect at least as extreme as the one in your sample data, assuming null hypothesis is true.\n",
    "\n",
    "(And again, the calculation does not measure in support for alternative hypothesis $H_A$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvqAXFNbNNW1"
   },
   "outputs": [],
   "source": [
    "ctr, samples, \\\n",
    "sample_se, SE, \\\n",
    "tStat, tCrit, \\\n",
    "confidence_interval, p_val = ind_t_test(group1 = campaign1,\n",
    "                                        group2= campaign2, \\\n",
    "                                        alpha = alpha,\\\n",
    "                                        output_bool = True, \\\n",
    "                                        state_conclusion=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7cuWsKCNNW3"
   },
   "source": [
    "### 2.3.2 Non-Manual Approach\n",
    "\n",
    "We will use the package StatModel, with function [Proportion Z-Test](https://tedboy.github.io/statsmodels_doc/doc/generated/statsmodels.stats.proportion.proportions_ztest.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "id": "RXZm4rFpNNW3",
    "outputId": "6b0b765a-207d-45e1-e1e9-94a145eb02f1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-047f3d1e4437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrial_successes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcampaign1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;34m,\u001b[0m               \u001b[0mcampaign2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcampaign1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcampaign1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0mcampaign2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'No Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcampaign2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Click'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'campaign1' is not defined"
     ]
    }
   ],
   "source": [
    "# Try out the Proportion Z-Test from the StatModel library \n",
    "# statsmodels.stats.proportion.proportions_ztest()\n",
    "\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "\n",
    "trial_successes = np.array([campaign1['Click'].sum()  , \\\n",
    "              campaign2['Click'].sum()])\n",
    "total = np.array([campaign1['No Click'].sum() + campaign1['Click'].sum(), \\\n",
    "              campaign2['No Click'].sum() + campaign2['Click'].sum()])\n",
    "\n",
    "zTest_statistic, zTest_pval = proportions_ztest(trial_successes, total, value=0, alternative='two-sided')\n",
    "\n",
    "print(f'StatModels\\'s Calculated t-statistic is {round(zTest_statistic,4)}' + \\\n",
    "      f' and p-value is {zTest_pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmaHHeDeNNW6"
   },
   "source": [
    "That's it, that's the test.\n",
    "\n",
    "![Awk](../Resources/Images/awkward_meme.png)\n",
    "\n",
    "Oh, but we should still respectively state our conclusion (for respect of the test)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZ2FugPvNNW7"
   },
   "outputs": [],
   "source": [
    "if zTest_pval < alpha:\n",
    "    print('\\nWe reject the Null Hypothesis H_0')\n",
    "    print('Therefore, we can say that there is a statistical ' + \\\n",
    "    'difference between the two campaigns.')\n",
    "\n",
    "else:\n",
    "    print('\\nWe fail to reject the Null Hypothesis H_0')\n",
    "    print('\\nTherefore, we can say that there is no statistical' + \\\n",
    "    ' significant difference between the two campaigns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abVQei6qNNW9"
   },
   "source": [
    "However, there can be some error in setup, calculation, introduction of bias, or another mistep in properly testing\n",
    "With that being said, remember...\n",
    "\n",
    "\n",
    "![Always be Testing](../Resources/Images/always_be_testing.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlGjtwrCNNW-"
   },
   "source": [
    "[“Ignorance more frequently begets confidence than does knowledge”](https://www.goodreads.com/quotes/24141-ignorance-more-frequently-begets-confidence-than-does-knowledge-it-is)\n",
    "\n",
    "― Charles Darwin, The Descent of Man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEoYUnIANNW_"
   },
   "source": [
    "# 3.0 Fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyMeN8GPNNW_"
   },
   "source": [
    "# 4.0 Additional Content [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx_NtqmdNNXB"
   },
   "source": [
    "### 4.1 Segmentation\n",
    "\n",
    "The type of users we are interested in depends on who our targeted audience is for this testing. \n",
    "(e.g. Do we simply sample from the overall population of this test, or are we evaluating performance between a particular group? \n",
    "\n",
    "From our previous mention of keywords, Google Ads enables us to selectively target our ads in a bid for certain types of keywords. If we win the bid, our ad is shown related to that keyword. \n",
    "\n",
    "> Note: If our segmentation is too specific, we lead into the impliciation of incorrectly rejecting our existing, null, hypothesis for another--something called the **Simpson's Paradox**. This is because the more refined our segmentation is, we target a specific case or decrease our # of observations such that we lead to those results.\n",
    "\n",
    "> Luckily, to best audit that one can identify if the culmination of different segments have the same results as the total sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq4k_G-BNNXD"
   },
   "source": [
    "That is, identify if segments $A_n$, where $n \\in {1,2,...n}$'s overall results hold true for sample $A$\n",
    "\n",
    "![Simpon's Paradox Example](../Resources/Images/simpons_paradox_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcqDHb2LNNXF"
   },
   "source": [
    "* Mind the possibility of the Simpson's Paradox\n",
    "    * Segmentation of groups in thus reducing sampling creates significance, but not from other groups from data before segmentation. Best way to validate this doesn't exist is to do the same test for combined groups.\n",
    "        * E.g. Splitting between new users and existing users\n",
    "* $\\alpha$ percent of the time, you'll reach significance due to chance \n",
    "    * You are running a tests with 20 variants, and you test each hypothesis separately:\n",
    "        * P(one significant result) = 1−P(no significant results) \n",
    "        * P(one significant result) = 1−(1−0.05)^20 = 0.64\n",
    "    * Avoid this by the Bonferroni Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB7-9sA6NNXG"
   },
   "source": [
    "## 4.2 Minimum Detectable Effect, Sample Size, and Duration\n",
    "\n",
    "### 4.2.1 MDE \n",
    "\n",
    "Assume the original ad's daily CTR performance was, on average, ~50%. That being said, we have a basis for what we know previous to doing anything in the test. \n",
    "> I.e. we have a basis measure to compare this baseline measure of an estimated ~50% CTR \n",
    "\n",
    "Originally, we would like to evaluate if there is a statistical significant difference in the ads performance, under assumption of the baseline measure. \n",
    "\n",
    "And hypothetically say we reject the original assumption $H_0$, such that we decide to commit to shifting to the new ad campaign, regardless of if it is statistically significantly different. Depending on the nature of the experimentation setup, business logic, costs, and so much more, is this a practical significance in realistically moving to a new implementation? The measuring criteria for this **practical significance** is understanding our **Minimum Dectable Effect (MDE)** for us to consider the new implementation.\n",
    "\n",
    "> An example of MDE is the consideration of cost of investment, change management, or risk.\n",
    "\n",
    "The MDE calculation for our case is defined as \n",
    "\n",
    "$\\text{MDE}:=t^* \\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\tfrac{\\hat{p}_1(1-\\hat{p}_2)}{n_2}}$\n",
    "\n",
    "Notice that with the Minimum Detectable Difference in mind, we need to have a few other considerations in our testing:\n",
    "\n",
    "1. Sample Size n\n",
    "    * Duration\n",
    "2. CTR, $\\hat{p}$\n",
    "\n",
    "Though we do not have this measure calculated for us, we can establish what is considered a MDE value for the test. Moreover, we can have a baseline measure $\\hat{p_A}$, from our CTR from campaign A. \n",
    "\n",
    "So, where does that leave the sample size? Given the above, we can still calculate that value + understand the time to reach that # of samples for the campaigns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuvSPuquNNXH"
   },
   "source": [
    "#### 4.3 Sample Size \n",
    "\n",
    "\n",
    "![Samples!](../Resources/Images/costco_samples.jpg)\n",
    "\n",
    "\n",
    "Assuming the two samples have an estimated equal amount of observations & CTR $\\hat{p}_1$ & $\\hat{p}_2$, We can compute variant A's sample size n to reach MDE as\n",
    "\n",
    "\n",
    "$\\text{MDE} =t^*  \\sqrt{\\tfrac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+\\tfrac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\implies$\n",
    "\n",
    "$ t^*  \\sqrt{\\tfrac{\\hat{p}(1-\\hat{p})}{n}+\\tfrac{\\hat{p}(1-\\hat{p})}{n}} =$\n",
    "\n",
    "$t^*  \\sqrt{2\\tfrac{\\hat{p}(1-\\hat{p})}{n}}$\n",
    "\n",
    "Re-formulating to solve for n, \n",
    "\n",
    "$ \\text{MDE} = t^*  \\sqrt{2\\tfrac{\\hat{p}(1-\\hat{p})}{n}} \\implies $ \n",
    "\n",
    "$(\\tfrac{\\text{MDE}}{t^*})^2   = 2\\tfrac{\\hat{p}(1-\\hat{p})}{n} \\implies $\n",
    "\n",
    "$n= 2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2$\n",
    "\n",
    "\n",
    "\n",
    "Remember, we have two variants A & B. Therefore, thet total numner of samples need is \n",
    "\n",
    "$n = n_1 + n_2 = (2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2) + (2 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2) $\n",
    "\n",
    "$=4 \\hat{p}(1-\\hat{p})(\\tfrac{t^* }{\\text{MDE}})^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiQecd4LNNXJ"
   },
   "source": [
    "\n",
    "##### Example \n",
    "\n",
    "Let t = 1.96, $\\hat{p} = .51$, and MDE = .05 What would the hypothetical MDE be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_z0WRHONNXL",
    "outputId": "52f0972d-8caa-4cd5-de2b-a07055057974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example's Estimated MDE is: 1536.0\n"
     ]
    }
   ],
   "source": [
    "t_est = 1.96 #{Enter value here}\n",
    "p_est = .51 #{Enter value here}\n",
    "mde = .05 #{Enter value here}\n",
    "\n",
    "# Construct the estimated sample size n_mde, from the formula above \n",
    "n_mde = (4) * (p_est * (1- p_est)) * (t_est/.05)**2\n",
    "\n",
    "print(f'Example\\'s Estimated MDE is: {round(n_mde, 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpAYlv4nNNXM"
   },
   "source": [
    "\n",
    "Keep in mind of the following:\n",
    "\n",
    "* Sampling is not linear. So if we want to detect an effect by some other MDE, we have to consider adding a multiple of 4 total observations into data collection.\n",
    "\n",
    "* Tests with smaller sample size can have lower power (I.e.. Tests with lower sample size can only detect large impact)\n",
    "\n",
    "* Also...there are tools out there to calcualted estimated sample size, like Evan Miller's calculator [here](http://www.evanmiller.org/ab-testing/sample-size.html#!51;80;5;5;0)\n",
    "    * E.g. This link will land us to the results for a sample size to be an estimated ~1,600 rows-- Close enough! ;D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jvEMHjANNXN"
   },
   "source": [
    "#### 4.4 Duration\n",
    "\n",
    "For duration, we identify it based on previous traffic history.\n",
    "\n",
    "> As an example, if we needed a combined 20,000 total observations for both groups, and we know we have 4,000 unique users search per week, then we would have to wait 5 weeks for our test to run, assuming traffic flow is at a steady stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiENgM-XNNXN"
   },
   "source": [
    "## 4.5 Data Collection\n",
    "\n",
    "### 4.5.1 System Setup\n",
    "\n",
    "Assume 3.2.7 assumptions hold true. Moreover, assume that our data collection and experiment would be run fairly from system  setup for the experiment.\n",
    "\n",
    "* In particular,  we are using the Goodle Ads system, and is currently known for being reliable. But how do we know no performance downgrade has occurred (and all hell breaks loose on the internet with a P0 to the next available engineer #HugOps)?\n",
    "\n",
    "#### 4.5.2 Fairness, and A/A Testing\n",
    "\n",
    "To test if our system is set up correctly, either in procedures, allocation of assignments, or other reasons, a tatic used to test if a tool runs experiments fairly is called **A/A testing**. \n",
    "\n",
    "\n",
    "![A, it's A!](../Resources/Images/a_a_test_meme.png)\n",
    "\n",
    "A/A Test tests two identical versions of a page against each other. In an this same-group comparison, the nature of the tool or other factors should show no difference in conversions between the two variant/groups. \n",
    "\n",
    "If there is a statistical significance in this tool, then something is afoot, and your actual implementation increases chances of having incorrect conclusions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jk9jTl6eNNXN"
   },
   "source": [
    "## 4.6 P-Hacking\n",
    "\n",
    "* Be weary on being incentivized to prove something is different\n",
    "    * Don't shape results, after the fact\n",
    "* During the test\n",
    "    * Don't peek\n",
    "        * Avoid t-test timelines, though some companies may have that option\n",
    "    * Refrain from stopping test at point of first statistical difference result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_63bJjXNNXO"
   },
   "source": [
    "## 4.7 Sample Size Matters\n",
    "\n",
    "Here’s how small effect sizes can still produce tiny p-values:\n",
    "\n",
    "You have a very large sample size. As the sample size increases, the hypothesis test gains greater statistical power to detect small effects. With a large enough sample size, the hypothesis test can detect an effect that is so miniscule that it is meaningless in a practical sense.\n",
    "\n",
    "The sample variability is very low. When your sample data have low variability, hypothesis tests can produce more precise estimates of the population’s effect. This precision allows the test to detect tiny effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVaPVnX8NNXP"
   },
   "source": [
    "## 4.8 Interpreting the probability $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtR_zglCNNXP",
    "outputId": "10ee8783-d085-4310-8f1e-160792ba37b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE 1: 0.012904003828843776\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n",
      "SE 1: 0.012906225887790229\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n",
      "SE 1: 0.012909872765245009\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n",
      "SE 1: 0.012901576131103775\n",
      "\n",
      "We fail to reject the Null Hypothesis H_0\n",
      "\n",
      "Therefore, we can say that there is no statistical significant difference between the two campaigns.\n",
      "SE 1: 0.01290535346854739\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n",
      "SE 1: 0.012905237954670447\n",
      "\n",
      "We fail to reject the Null Hypothesis H_0\n",
      "\n",
      "Therefore, we can say that there is no statistical significant difference between the two campaigns.\n",
      "SE 1: 0.012907362240210043\n",
      "\n",
      "We fail to reject the Null Hypothesis H_0\n",
      "\n",
      "Therefore, we can say that there is no statistical significant difference between the two campaigns.\n",
      "SE 1: 0.012895474371238122\n",
      "\n",
      "We fail to reject the Null Hypothesis H_0\n",
      "\n",
      "Therefore, we can say that there is no statistical significant difference between the two campaigns.\n",
      "SE 1: 0.012904133704618325\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n",
      "SE 1: 0.012902913107770147\n",
      "\n",
      "We reject the Null Hypothesis H_0\n",
      "Therefore, we can say that there is a statistical difference between the two campaigns.\n"
     ]
    }
   ],
   "source": [
    "# Demo on Alpha value\n",
    "\n",
    "for i in range(10):\n",
    "    expressCamp1, expressCamp2 = express_campaign_df_generator([campaign1_name,campaign2_name], \\\n",
    "                                      weightMatrix = [[.5, .5],[.53, .47]],\\\n",
    "                                      sample_size = 3000)\n",
    "    ind_t_test(group1 = expressCamp1, \\\n",
    "               group2 = expressCamp2, \\\n",
    "               alpha=0.05, \\\n",
    "               output_bool=False, \\\n",
    "               state_conclusion=True, \\\n",
    "              express = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeyFn_e5NNXR"
   },
   "source": [
    "## 4.9 Practical vs Significance\n",
    "\n",
    "Statistical significance indicates only that you have sufficient evidence to conclude that an effect exists. It is a mathematical definition that does not know anything about the subject area and what constitutes an important effect.\n",
    "\n",
    "\n",
    "    \n",
    "* Note: Novelty Effect or Change Aversion: Post-test Cohort analysis may be helpful in evaluation if testing was valid for users, after some timeframe after the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60OnUJYtNNXT"
   },
   "source": [
    "## 4.10 Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04k4ZCQaNNXT"
   },
   "source": [
    "The Central Limit Theorem (CLT) implies a sample of independent random variables, their sums tends towards to a normal distribution even if the original variables themselves aren't normally distributed, also the sample mean tends towards to a normal distribution (sum and mean are equivalent).\n",
    "\n",
    "\n",
    "That is, For large values of n, the distributions of the count $X$ and the sample proportion are approximately normal due to the Central Limit Theorem, as it approximates the normal distribution like \n",
    "\n",
    "$\\bar{X}$ ~ $N(\\tfrac{np}{p},\\tfrac{np(1-p)}{n^2}) = N(n, \\tfrac{p(1-p)}{n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMHRNpe2NNXU"
   },
   "source": [
    "\n",
    "\n",
    "## 4.11 On the note of the Decision Tree image...\n",
    "\n",
    "Differences between z & t tests:\n",
    "\n",
    "* z-test: A z-test assumes observations are independently drawn from Normal Distribution with unknown mean and known variance. Z-test is used when we have quantitative data.\n",
    "\n",
    "* t-test: a t-test assumes observations are independently drawn from Normal distribution with unknown mean and unknown variance. With a t-test, we do not know the population variance.\n",
    "\n",
    "## 4.12  Ethics\n",
    "\n",
    "    * Do your users know about being tested on?\n",
    "    * Privacy concerns \n",
    "    * Post-testing effects on participants\n",
    "        * IRB (Institutional Review Board) is not necessary unless farmful actions happen to test participants\n",
    "        * Formal and regulated tests require knowledge of privacy, choice, risk, and informed consent\n",
    "    \n",
    "\n",
    "### 4.13 Other: Welche's T-test (T test with unequal variance or observations)\n",
    "\n",
    "Equal or unequal sample sizes, unequal variances\n",
    "This test, also known as Welch's t-test, is used only when two population variances are not assumed to be equal (the two sample sizes may or may not be equal) and hence must be estimated seperately. The t-stat to test whether population means are different is\n",
    "\n",
    "$t= \\tfrac{\\bar{X}_1 - \\bar{X}_2}{s_{\\bar{\\Delta}}}$\n",
    "\n",
    "where $s_{\\bar{\\Delta}} = \\sqrt{\\tfrac{s^2_1}{n_1} + \\tfrac{s^2_2}{n_2}}$\n",
    "\n",
    "d.f. = $\\tfrac{(\\tfrac{s^2_1}{n_1} + \\tfrac{s^2_2}{n_2})^2}{\\tfrac{(\\tfrac{s^2_1}{n_1})^2}{n_1 - 1} + \\tfrac{(\\tfrac{s^2_2}{n_2})^2}{n_2 - 1}}$\n",
    "\n",
    "This d.f. is known as the Welch-Satterthwaite equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3WuY4mSWNNXU"
   },
   "source": [
    "<a id='appendix-chi-square-test'></a>\n",
    "\n",
    "\n",
    "\n",
    "# 5.0 The Chai-err umm....Chi-Squared Test! [Optional]\n",
    "\n",
    "\n",
    "## 5.1 $\\chi^2$ Testing [Optional]\n",
    "\n",
    "\n",
    "* Chi-Square Goodness of Fit Test\n",
    "    * $\\chi^2$ Test that determines if a sample data matches a population. For more details on this type, see: Goodness of Fit Test.\n",
    "\n",
    "* Chi-Square Test for Independence\n",
    "\n",
    "    * $\\chi^2$ Test that compares two variables in a contingency table to see if they are related. In a more general sense, it tests to see whether distributions of categorical variables differ from each another.\n",
    "    \n",
    "> Note: Chi Square is cool because it works with more than 2 samples\n",
    "\n",
    "> Note: If we have a  small sample size, then Chi Square may have more errors, and thus one would have to work with a Fischer's Exact Test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-nA32nlNNXU"
   },
   "source": [
    "## 5.2 Applying the $\\chi^2$ Test \n",
    "\n",
    "### 5.2.1 The Manual Approach\n",
    "Equivalently, we can use a similiar process and test statistic in evaluation of proportions, seen in section 3.0+. \n",
    "\n",
    "Let us have a 2x2 frequency table where columns are two groups of respondents and rows are the two responses \"Clicks\" (our successes) and \"No Clicks\". \n",
    "\n",
    "\n",
    "|    _    | Click | No Click | \n",
    "|--------|--------|-------------|\n",
    "| Test 1 | X_1    | Y_1         |\n",
    "| Test 2 | X_2    | Y_2         |\n",
    "| Total  | x      | x           | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ddpx_2INNXV"
   },
   "source": [
    "State the Hypothesis:\n",
    "\n",
    "$H_0: \\bar{p_1} - \\bar{p_2} = 0$ \n",
    "\n",
    "$H_1: \\bar{p_1} - \\bar{p_2} \\neq 0$\n",
    "\n",
    "Degrees of freedom = $(x_1 - 1) * (x_2 - 1)$\n",
    "\n",
    "Test Statistic:\n",
    "\n",
    "$\\chi^2= \\tfrac{(O-E)^2}{E}$, \n",
    "\n",
    "where $O$ are the Observed values and $E$ are the Expected values.\n",
    "\n",
    "> Note:\n",
    "Expected values are calculated as such:\n",
    "> For the top left region, it would be $\\tfrac{( x_{Clicks} * (X_1 + Y_1)}{x_T }$\n",
    "\n",
    "|    _    | Clicks | No Clicks | Total (Impressions) |\n",
    "|--------|--------|-------------|-----|\n",
    "| Test 1 | $Y_1$    | $Y_1$         | $X_1 + Y_1$  | \n",
    "| Test 2 | $X_2 $   | $Y_2 $        | $X_2 + Y_2 $  |\n",
    "| Total  | $x_{Clicks}$      | $x_{No Clicks}$        | $x_T$   | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsKUxQyBNNXV"
   },
   "outputs": [],
   "source": [
    "df = [ [campaign1['Click'].sum(), \\\n",
    "        campaign1['No Click'].sum(),\\\n",
    "        campaign1['No Click'].sum()+campaign1['Click'].sum()], \\\n",
    "      [campaign2['Click'].sum(), \\\n",
    "       campaign2['No Click'].sum(), \\\n",
    "        campaign2['No Click'].sum()+campaign2['Click'].sum()] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94apH7oxNNXW"
   },
   "outputs": [],
   "source": [
    "twoByTwo_df = pd.DataFrame(df,index=['Campaign 1', 'Campaign 2'] ,\\\n",
    "                           columns=['Click','No Click','Impressions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT_oENTaNNXX",
    "outputId": "a61c87a0-b3e3-4a64-a8ac-e4968c6ff84c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Click</th>\n",
       "      <th>No Click</th>\n",
       "      <th>Impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Campaign 1</th>\n",
       "      <td>824.0</td>\n",
       "      <td>776.0</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campaign 2</th>\n",
       "      <td>879.0</td>\n",
       "      <td>721.0</td>\n",
       "      <td>1600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Click  No Click  Impressions\n",
       "Campaign 1  824.0     776.0       1600.0\n",
       "Campaign 2  879.0     721.0       1600.0"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twoByTwo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2Moy5-YNNXY"
   },
   "outputs": [],
   "source": [
    "expectedClicksSeries = (twoByTwo_df['Impressions']/twoByTwo_df['Impressions'].sum()) * twoByTwo_df['Click'].sum() \n",
    "\n",
    "expectedNonClicksSeries = (twoByTwo_df['Impressions']/twoByTwo_df['Impressions'].sum()) * twoByTwo_df['No Click'].sum() \n",
    "\n",
    "expectedDf = pd.concat([expectedClicksSeries,expectedNonClicksSeries],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJrbLlj-NNXZ",
    "outputId": "bf417ee8-2d9c-490e-cee3-1834089af5b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Click</th>\n",
       "      <th>No Click</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Campaign 1</th>\n",
       "      <td>851.5</td>\n",
       "      <td>748.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campaign 2</th>\n",
       "      <td>851.5</td>\n",
       "      <td>748.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Click  No Click\n",
       "Campaign 1  851.5     748.5\n",
       "Campaign 2  851.5     748.5"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectedDf.columns =['Click', 'No Click']\n",
    "expectedDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxgmtzW-NNXb"
   },
   "outputs": [],
   "source": [
    "chiSquareStatistic = ((twoByTwo_df[['Click','No Click']]-expectedDf)**2 / expectedDf).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzKlD0owNNXe",
    "outputId": "a4087f48-0460-4033-da94-717c1661156d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7969852407888784"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chiSquareStatistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0VUX3UqNNXf"
   },
   "source": [
    "### 5.2.2 The SciPy Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xedHtmsYNNXg",
    "outputId": "8324fb26-c635-44bd-e707-39090e57b59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Table: \n",
      " [[ 851.5  748.5 1600. ]\n",
      " [ 851.5  748.5 1600. ]] \n",
      "\n",
      "Degrees of Freedom: 2\n",
      "\n",
      "probability=0.95, critical=5.99, stat=0.05 \n",
      "\n",
      "Decision:\n",
      "For significance level 0.05,\n",
      "\n",
      "We fail to reject the Null Hypothesis, H_0\n",
      "\n",
      "for p = 0.95 < 5.991464547107979.\n"
     ]
    }
   ],
   "source": [
    "chiSquareStat, pVal, dof, expected = scipy.stats.chi2_contingency(twoByTwo_df)\n",
    "\n",
    "print('Expected Table: \\n',expected,'\\n')\n",
    "\n",
    "# interpret test-statistic\n",
    "prob = 0.95\n",
    "alpha = 1.0 - prob\n",
    "\n",
    "criticalVal = scipy.stats.chi2.ppf(prob, dof)\n",
    "\n",
    "print(f'Degrees of Freedom: {dof}\\n')\n",
    "print('probability=%.2f, critical=%.2f, stat=%.2f \\n' % (prob, criticalVal, alpha))\n",
    "\n",
    "print('Decision:')\n",
    "print(f'For significance level {round(alpha,2)},\\n')\n",
    "\n",
    "if abs(chiSquareStat) >= criticalVal:\n",
    "    print('We reject the Null Hypothesis, H_0\\n')\n",
    "    print(f'for p = {prob} >= {criticalVal}.')\n",
    "else:\n",
    "    print('We fail to reject the Null Hypothesis, H_0\\n')\n",
    "    print(f'for p = {prob} < {criticalVal}.')\n",
    "\n",
    "    \n",
    "### Alternatively can say:    \n",
    "# if p <= alpha:\n",
    "#     print('We reject the Null Hypothesis, H_0.')\n",
    "#     print(f'for p = {p} >= {alpha}.')\n",
    "\n",
    "# else:\n",
    "#     print('We fail to reject the Null Hypothesis, H_0.')\n",
    "#     print(f'for p = {p} < {alpha}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgGX8GIjNNXi"
   },
   "source": [
    "# 6.0 Resources\n",
    "\n",
    "* [Understanding A/B Testing and Statistics Behind it](https://byrony.github.io/understanding-ab-testing-and-statistics-behind.html)\n",
    "            \n",
    "    \n",
    "* [Difference between Mcnemar Test and Chi Square Test](https://stats.stackexchange.com/questions/76875/what-is-the-difference-between-mcnemars-test-and-the-chi-squared-test-and-how/141450#141450)\n",
    "\n",
    "\n",
    "\n",
    "* [“Ignorance more frequently begets confidence than does knowledge” quote](https://www.goodreads.com/quotes/24141-ignorance-more-frequently-begets-confidence-than-does-knowledge-it-is)\n",
    "\n",
    "* [Power Minimal Detectable Effect and Bucket Size Estimation](https://blog.twitter.com/engineering/en_us/a/2016/power-minimal-detectable-effect-and-bucket-size-estimation-in-ab-tests.html)\n",
    "\n",
    "\n",
    "* [Hypothesis Testing: Assumptions & Conditions + More](https://www.b-g.k12.ky.us/userfiles/1525/Classes/33386/Conditions%20of%20Tests.08-09%20rev%201.pdf?source=post_page---------------------------)\n",
    "\n",
    "\n",
    "* [What's up with the N-1 for the denominator in sample calculations?](https://en.wikipedia.org/wiki/Bessel%27s_correction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PXYnqIKBNNWI",
    "2jvEMHjANNXN",
    "wiENgM-XNNXN",
    "Jk9jTl6eNNXN",
    "v_63bJjXNNXO",
    "hVaPVnX8NNXP",
    "XeyFn_e5NNXR",
    "60OnUJYtNNXT",
    "RMHRNpe2NNXU",
    "x-nA32nlNNXU",
    "G0VUX3UqNNXf"
   ],
   "name": "1-ABTesting-Presentation-2019-Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
